## NLP Research Papers

[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity)  [![GitHub contributors](https://img.shields.io/github/contributors/Naereen/StrapDown.js.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/contributors/) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)

This repository contains landmark research papers in Natural Language Processing that came out in this century.

## How to Read a Paper?
Reading a paper is not the same as reading a blogpost or a novel. Here are a few handy resources to help you get started.

* [How to read an academic article](https://organizationsandmarkets.com/2010/08/31/how-to-read-an-academic-article/)<br>
* [Advice on reading academic papers](https://www.cc.gatech.edu/~akmassey/posts/2012-02-15-advice-on-reading-academic-papers.html)<br>
* [How to read and understand a scientific paper](https://violentmetaphors.com/2013/08/25/how-to-read-and-understand-a-scientific-paper-2/)<br>
* [Should I Read Papers?](http://michaelrbernste.in/2014/10/21/should-i-read-papers.html)<br>
* [The Refreshingly Rewarding Realm of Research Papers](https://www.youtube.com/watch?v=8eRx5Wo3xYA)<br>

## List of Research Papers

### :point_right: Machine Translation 

* [Sequence to Sequence Learning with Neural Network](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
* [Learning Phase Representations using RNN Encoder-Decoder for statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
* [Google Machine Translation Blog](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)
* [Attention Model(Neural Machine Translation By Jointly learning to Align and Translate)](https://arxiv.org/pdf/1409.0473.pdf)
* [Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381.pdf)
* [MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning](https://arxiv.org/abs/1911.09483)
* [Scaling Neural Machine Translation](https://arxiv.org/abs/1806.00187)
* [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)

### Others

* [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430)
* [Improving Neural Language Modeling via Adversarial Training](http://proceedings.mlr.press/v97/wang19f/wang19f.pdf)
* [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

* [Efficient Estimation of Word Representations in Vector Space, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%2C%20Google.pdf)<br>
* [Distributed Representations of Words and Phrases, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/Distributed%20Representations%20of%20Words%20and%20Phrases%2C%20Google.pdf)<br>
* [Distributed Representations of Sentences and Documents, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/Distributed%20Representations%20of%20Sentences%20and%20Documents%2C%20Google.pdf)<br>
* [Enriching Word Vectors with Subword Information, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Enriching%20Word%20Vectors%20with%20Subword%20Information%2C%20Facebook.pdf)<br>
* [Bag of Tricks for Efficient Text Classification, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification%2C%20facebook.pdf)<br>
* [Hierarchical Probabilistic Neural Network Language Model](https://github.com/Akshat4112/NLP-research-papers/blob/master/Hierarchical%20Probabilistic%20Neural%20Network%20Language%20Model.pdf)<br>
* [A Scalable Hierarchical Distributed Language Model](https://github.com/Akshat4112/NLP-research-papers/blob/master/A%20Scalable%20Hierarchical%20Distributed%20Language%20Model.pdf)<br>
* [BERT Pre training of Deep Bidirectional Transformers for Language Understanding, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/BERT%20Pre%20training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Uderstanding%2C%20Google.pdf)<br>
* [Language Models are Unsupervised Multitask Learners, OpenAI](https://github.com/Akshat4112/NLP-research-papers/blob/master/Language%20Models%20are%20Unsupervised%20Multitask%20Learners%2C%20openai.pdf)<br>
* [Wav2Letter, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Wav2Letter%2C%20Facebook.pdf)<br>
* [Misspelling Oblivious Word Embeddings, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Misspelling%20Oblivious%20Word%20Embeddings%2C%20Facebook.pdf)<br>

## Credits
* :man: [Akshat Gupta](https://github.com/Akshat4112/)
* :woman: [Ridhima Garg](https://github.com/ridhimagarg)
