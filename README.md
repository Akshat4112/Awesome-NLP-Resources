## NLP Research Papers
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Akshat4112/Awesome-NLP-Resources/graphs/commit-activity)  [![GitHub contributors](https://img.shields.io/github/contributors/Nareem/StrapDown.js.svg)](https://github.com/Akshat4112/Awesome-NLP-Resources/graphs/contributors) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/) 

This repository contains landmark research papers and blogs in Natural Language Processing that came out in this century.

## Contents

* [How to Read a Paper?](#How-to-Read-a-Paper?)
* [List of blogs](#List-of-blogs)
    * [Machine Translation](#Machine-Translation)
    * [Language Models](#Language-Models)
    * [Image to Text](#Image-to-text)
    * [Transformers](#Transformers)
* [List of Reasearch Papers](#List-of-Reasearch-Papers)
    * [Machine Translation](#Machine-Translation)
    * [Image to Text](#Image-to-text)
    * [Transformers](#Transformers)

## How to Read a Paper? :page_with_curl:
Reading a paper is not the same as reading a blogpost or a novel. Here are a few handy resources to help you get started.

* [How to read an academic article](https://organizationsandmarkets.com/2010/08/31/how-to-read-an-academic-article/)<br>
* [Advice on reading academic papers](https://www.cc.gatech.edu/~akmassey/posts/2012-02-15-advice-on-reading-academic-papers.html)<br>
* [How to read and understand a scientific paper](https://violentmetaphors.com/2013/08/25/how-to-read-and-understand-a-scientific-paper-2/)<br>
* [Should I Read Papers?](http://michaelrbernste.in/2014/10/21/should-i-read-papers.html)<br>
* [The Refreshingly Rewarding Realm of Research Papers](https://www.youtube.com/watch?v=8eRx5Wo3xYA)<br>

## List of Research Papers

### Machine Translation 

* [Sequence to Sequence Learning with Neural Network](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - LSTMN based approach for sequence problems.
* [Learning Phase Representations using RNN Encoder-Decoder for statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
* [Attention Model(Neural Machine Translation By Jointly learning to Align and Translate)](https://arxiv.org/pdf/1409.0473.pdf) - Attention model architecture modified version for encoder decoder models (Don't confuse with [<i><b>Attention is all you need</b> paper</i>](#Transformers) i.e, for transformers concept)
* [Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381.pdf)
* [MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning](https://arxiv.org/abs/1911.09483)
* [Scaling Neural Machine Translation](https://arxiv.org/abs/1806.00187)
* [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849)
* [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)- Modified Attention model with convolutional layer


### Language Models
* [Scalable Hierarchical Distributed Language Model](https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf)
* [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf) - fastText(by Facebook AI Research) trained on billion of words for text classification. 


### Word Embeddings
* [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) - Sentence/Document to vectors by Tomas Mikolov by Google
* [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)- WOrd2Vec representation by Tomas Mikolov(Google)
* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) - High quality vector representation from huge data sets by Tomas Mikolov(Google)

### Image to Text

* [Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
* [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)


### Transformers
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) - Multimodal Recurrent Neural Network architecture for image description by [Andrej Kaparthy <img src="andreaj.svg" width="20" height="20"> ](http://karpathy.github.io/) and Le-Fei-fei


## List of blogs

### Machine Translation
* [Google Machine Translation Blog](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)
* [Email AutoReply and Auto Suggestion](https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html)
* [Find Code errors and repair](https://medium.com/@martin.monperrus/sequence-to-sequence-learning-program-repair-e39dc5c0119b)



### Image to Text
* [Image Captioning Using Keras](https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8)


### Transformers
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Transformers Research paper core details explained by [Jalammar](http://jalammar.github.io/)
* [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) - BERT is explained by [Jalammar](http://jalammar.github.io/)
* [A Visual Guide to Using BERT for the First Time :boom:](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) - Very beautifully explained BERT architecture with the help of visuals. 

### [Back to Top](#Contents)

### Others

* [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430)
* [Improving Neural Language Modeling via Adversarial Training](http://proceedings.mlr.press/v97/wang19f/wang19f.pdf)
* [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)


* [Efficient Estimation of Word Representations in Vector Space, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%2C%20Google.pdf)<br>
* [Distributed Representations of Words and Phrases, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/Distributed%20Representations%20of%20Words%20and%20Phrases%2C%20Google.pdf)<br>
* [Distributed Representations of Sentences and Documents, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/Distributed%20Representations%20of%20Sentences%20and%20Documents%2C%20Google.pdf)<br>
* [Enriching Word Vectors with Subword Information, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Enriching%20Word%20Vectors%20with%20Subword%20Information%2C%20Facebook.pdf)<br>
* [Bag of Tricks for Efficient Text Classification, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification%2C%20facebook.pdf)<br>
* [Hierarchical Probabilistic Neural Network Language Model](https://github.com/Akshat4112/NLP-research-papers/blob/master/Hierarchical%20Probabilistic%20Neural%20Network%20Language%20Model.pdf)<br>
* [A Scalable Hierarchical Distributed Language Model](https://github.com/Akshat4112/NLP-research-papers/blob/master/A%20Scalable%20Hierarchical%20Distributed%20Language%20Model.pdf)<br>
* [BERT Pre training of Deep Bidirectional Transformers for Language Understanding, Google](https://github.com/Akshat4112/NLP-research-papers/blob/master/BERT%20Pre%20training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Uderstanding%2C%20Google.pdf)<br>
* [Language Models are Unsupervised Multitask Learners, OpenAI](https://github.com/Akshat4112/NLP-research-papers/blob/master/Language%20Models%20are%20Unsupervised%20Multitask%20Learners%2C%20openai.pdf)<br>
* [Wav2Letter, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Wav2Letter%2C%20Facebook.pdf)<br>
* [Misspelling Oblivious Word Embeddings, Facebook](https://github.com/Akshat4112/NLP-research-papers/blob/master/Misspelling%20Oblivious%20Word%20Embeddings%2C%20Facebook.pdf)<br>

## Credits
* :man: [Akshat Gupta](https://github.com/Akshat4112/)
* :woman: [Ridhima Garg](https://github.com/ridhimagarg)
